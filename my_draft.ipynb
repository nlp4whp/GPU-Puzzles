{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPU Puzzles\n",
    "- by [Sasha Rush](http://rush-nlp.com) - [srush_nlp](https://twitter.com/srush_nlp)"
   ],
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://github.com/srush/GPU-Puzzles/raw/main/cuda.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPU architectures are critical to machine learning, and seem to be\n",
    "becoming even more important every day. However, you can be an expert\n",
    "in machine learning without ever touching GPU code. It is hard to gain\n",
    "intuition working through abstractions. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is an attempt to teach beginner GPU programming in a\n",
    "completely interactive fashion. Instead of providing text with\n",
    "concepts, it throws you right into coding and building GPU\n",
    "kernels. The exercises use NUMBA which directly maps Python\n",
    "code to CUDA kernels. It looks like Python but is basically\n",
    "identical to writing low-level CUDA code. \n",
    "In a few hours, I think you can go from basics to\n",
    "understanding the real algorithms that power 99% of deep learning\n",
    "today. If you do want to read the manual, it is here:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[NUMBA CUDA Guide](https://numba.readthedocs.io/en/stable/cuda/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I recommend doing these in Colab, as it is easy to get started.  Be\n",
    "sure to make your own copy, turn on GPU mode in the settings (`Runtime / Change runtime type`, then set `Hardware accelerator` to `GPU`), and\n",
    "then get to coding."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/GPU-Puzzles/blob/main/GPU_puzzlers.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(If you are into this style of puzzle, also check out my [Tensor\n",
    "Puzzles](https://github.com/srush/Tensor-Puzzles) for PyTorch.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -qqq git+https://github.com/danoneata/chalk@srush-patch-1\n",
    "!wget -q https://github.com/srush/GPU-Puzzles/raw/main/robot.png https://github.com/srush/GPU-Puzzles/raw/main/lib.py\n",
    "\n",
    "### NOTE 如果无法通过pip安装chalk, 可以先安装内部自定义依赖planar, 然后通过git clone项目, 再通过chalk/chalk导入\n",
    "\n",
    "\"\"\" reference:\n",
    "- [并行前缀归约（Parallel Prefix Reduction）](https://zhuanlan.zhihu.com/p/469385277)\n",
    "- [CUDA高性能计算经典问题（二）—— 前缀和（Prefix Sum）](https://zhuanlan.zhihu.com/p/423992093)\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "import math\n",
    "import numba\n",
    "import numpy as np\n",
    "import warnings\n",
    "from lib import CudaProblem, Coord"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_next_cell": 2
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 1: Map\n",
    "\n",
    "Implement a \"kernel\" (GPU function) that adds 10 to each position of vector `a`\n",
    "and stores it in vector `out`.  You have 1 thread per position."
   ],
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Warning** This code looks like Python but it is really CUDA! You cannot use\n",
    "standard python tools like list comprehensions or ask for Numpy properties\n",
    "like shape or size (if you need the size, it is given as an argument).\n",
    "The puzzles only require doing simple operations, basically\n",
    "+, *, simple array indexing, for loops, and if statements.\n",
    "You are allowed to use local variables. \n",
    "If you get an\n",
    "error it is probably because you did something fancy :). "
   ],
   "metadata": {
    "lines_to_next_cell": 2
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Tip: Think of the function `call` as being run 1 time for each thread.\n",
    "The only difference is that `cuda.threadIdx.x` changes each time.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_spec(a):\n",
    "    return a + 10\n",
    "\n",
    "\n",
    "def map_test(cuda):\n",
    "    def call(out, a) -> None:\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 1 lines)\n",
    "        out[local_i] = a[local_i] + 10\n",
    "\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 4\n",
    "out = np.zeros((SIZE,))\n",
    "a = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Map\", map_test, [a], out, threadsperblock=Coord(SIZE, 1), spec=map_spec\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def map2D(out, a, max_x, max_y):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < max_x and y < max_y:\n",
    "        out[x, y] = a[x, y] + 10\n",
    "\n",
    "arr = np.random.random((3, 5))\n",
    "out = np.zeros((3, 5))\n",
    "\n",
    "map2D[(1, 1), (16, 16)](out, arr, 3, 5)\n",
    "out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 2 - Zip\n",
    "\n",
    "Implement a kernel that adds together each position of `a` and `b` and stores it in `out`.\n",
    "You have 1 thread per position."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def zip_spec(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def zip_test(cuda):\n",
    "    def call(out, a, b) -> None:\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 1 lines)\n",
    "        out[local_i] = a[local_i] + b[local_i]\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 4\n",
    "out = np.zeros((SIZE,))\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Zip\", zip_test, [a, b], out, threadsperblock=Coord(SIZE, 1), spec=zip_spec\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_next_cell": 0
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 3 - Guards\n",
    "\n",
    "Implement a kernel that adds 10 to each position of `a` and stores it in `out`.\n",
    "You have more threads than positions."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_guard_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 2 lines)\n",
    "        if local_i < size:\n",
    "            out[local_i] = a[local_i] + 10\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 4\n",
    "out = np.zeros((SIZE,))\n",
    "a = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Guard\",\n",
    "    map_guard_test,\n",
    "    [a],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(8, 1),\n",
    "    spec=map_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 4 - Map 2D\n",
    "\n",
    "Implement a kernel that adds 10 to each position of `a` and stores it in `out`.\n",
    "Input `a` is 2D and square. You have more threads than positions."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_2D_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        local_i = cuda.threadIdx.x\n",
    "        local_j = cuda.threadIdx.y\n",
    "        # FILL ME IN (roughly 2 lines)\n",
    "        if local_i < size and local_j < size:\n",
    "            out[local_i, local_j] = a[local_i, local_j] + 10\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 2\n",
    "out = np.zeros((SIZE, SIZE))\n",
    "a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "problem = CudaProblem(\n",
    "    \"Map 2D\", map_2D_test, [a], out, [SIZE], threadsperblock=Coord(3, 3), spec=map_spec\n",
    ")\n",
    "# problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 5 - Broadcast\n",
    "\n",
    "Implement a kernel that adds `a` and `b` and stores it in `out`.\n",
    "Inputs `a` and `b` are vectors. You have more threads than positions."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def broadcast_test(cuda):\n",
    "    def call(out, a, b, size) -> None:\n",
    "        local_i = cuda.threadIdx.x\n",
    "        local_j = cuda.threadIdx.y\n",
    "        # FILL ME IN (roughly 2 lines)\n",
    "        if local_i < size and local_j < size:\n",
    "            out[local_i, local_j] = a[local_i, 0] + b[0, local_j]\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 2\n",
    "out = np.zeros((SIZE, SIZE))\n",
    "a = np.arange(SIZE).reshape(SIZE, 1)\n",
    "b = np.arange(SIZE).reshape(1, SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Broadcast\",\n",
    "    broadcast_test,\n",
    "    [a, b],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(3, 3),\n",
    "    spec=zip_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 6 - Blocks\n",
    "\n",
    "Implement a kernel that adds 10 to each position of `a` and stores it in `out`.\n",
    "You have fewer threads per block than the size of `a`."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Tip: A block is a group of threads. The number of threads per block is limited, but we can\n",
    "have many different blocks. Variable `cuda.blockIdx` tells us what block we are in.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_block_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 2 lines)\n",
    "        if i < size:\n",
    "            out[i] = a[i] + 10\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 9\n",
    "out = np.zeros((SIZE,))\n",
    "a = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Blocks\",\n",
    "    map_block_test,\n",
    "    [a],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(4, 1),\n",
    "    blockspergrid=Coord(3, 1),\n",
    "    spec=map_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 7 - Blocks 2D\n",
    "\n",
    "Implement the same kernel in 2D.  You have fewer threads per block\n",
    "than the size of `a` in both directions."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def map_block2D_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 4 lines)\n",
    "        j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "        if i < size and j < size:\n",
    "            out[i, j] = a[i, j] + 10\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 5\n",
    "out = np.zeros((SIZE, SIZE))\n",
    "a = np.ones((SIZE, SIZE))\n",
    "\n",
    "problem = CudaProblem(\n",
    "    \"Blocks 2D\",\n",
    "    map_block2D_test,\n",
    "    [a],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(3, 3),\n",
    "    blockspergrid=Coord(2, 2),\n",
    "    spec=map_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 8 - Shared\n",
    "\n",
    "Implement a kernel that adds 10 to each position of `a` and stores it in `out`.\n",
    "You have fewer threads per block than the size of `a`."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Warning**: Each block can only have a *constant* amount of shared\n",
    " memory that threads in that block can read and write to. This needs\n",
    " to be a literal python constant not a variable. After writing to\n",
    " shared memory you need to call `cuda.syncthreads` to ensure that\n",
    " threads do not cross."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(This example does not really need shared memory or syncthreads, but it is a demo.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TPB = 4\n",
    "def shared_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        shared = cuda.shared.array(TPB, numba.float32)\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "\n",
    "        if i < size:\n",
    "            shared[local_i] = a[i]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        # FILL ME IN (roughly 2 lines)\n",
    "        if i < size:\n",
    "            out[i] = shared[local_i] + 10\n",
    "\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 8\n",
    "out = np.zeros(SIZE)\n",
    "a = np.ones(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Shared\",\n",
    "    shared_test,\n",
    "    [a],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(TPB, 1),\n",
    "    blockspergrid=Coord(2, 1),\n",
    "    spec=map_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 9 - Pooling\n",
    "\n",
    "Implement a kernel that sums together the last 3 position of `a` and stores it in `out`.\n",
    "You have 1 thread per position. You only need 1 global read and 1 global write per thread."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Tip: Remember to be careful about syncing.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pool_spec(a):\n",
    "    out = np.zeros(*a.shape)\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = a[max(i - 2, 0) : i + 1].sum()\n",
    "    return out\n",
    "\n",
    "\n",
    "TPB = 8\n",
    "def pool_test(cuda):\n",
    "    def call(out, a, size) -> None:\n",
    "        shared = cuda.shared.array(TPB, numba.float32)\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 8 lines)\n",
    "        if i < size:\n",
    "            shared[i] = a[local_i]\n",
    "            cuda.syncthreads()        \n",
    "        if i == 0:\n",
    "            out[i] = shared[i]\n",
    "        elif i == 1:\n",
    "            out[i] = shared[i] + shared[i - 1]\n",
    "        else:\n",
    "            out[i] = shared[i - 2] + shared[i - 1] + shared[i]\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 8\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Pooling\",\n",
    "    pool_test,\n",
    "    [a],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(TPB, 1),\n",
    "    blockspergrid=Coord(1, 1),\n",
    "    spec=pool_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 10 - Dot Product\n",
    "\n",
    "Implement a kernel that computes the dot-product of `a` and `b` and stores it in `out`.\n",
    "You have 1 thread per position. You only need 2 global reads and 1 global write per thread."
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Note: For this problem you don't need to worry about number of shared reads. We will\n",
    " handle that challenge later.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dot_spec(a, b):\n",
    "    return a @ b\n",
    "\n",
    "TPB = 8\n",
    "def dot_test(cuda):\n",
    "    def call(out, a, b, size) -> None:\n",
    "        shared = cuda.shared.array(TPB, numba.float32)\n",
    "\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 9 lines)\n",
    "        if i < size:\n",
    "            shared[i] = a[local_i] * b[local_i]\n",
    "            cuda.syncthreads()\n",
    "        if i == 0:  # atomic !!\n",
    "            for j in range(size):\n",
    "                shared[0] = shared[0] + shared[j]\n",
    "            cuda.syncthreads()\n",
    "        out[0] = shared[0]\n",
    "    return call\n",
    "\n",
    "\n",
    "SIZE = 8\n",
    "out = np.zeros(1)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Dot\",\n",
    "    dot_test,\n",
    "    [a, b],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    threadsperblock=Coord(SIZE, 1),\n",
    "    blockspergrid=Coord(1, 1),\n",
    "    spec=dot_spec,\n",
    ")\n",
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 11 - 1D Convolution\n",
    "\n",
    "Implement a kernel that computes a 1D convolution between `a` and `b` and stores it in `out`.\n",
    "You need to handle the general case. You only need 2 global reads and 1 global write per thread."
   ],
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def conv_spec(a, b):\n",
    "    out = np.zeros(*a.shape)\n",
    "    lens = b.shape[0]\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = sum([a[i + j] * b[j] for j in range(lens) if i + j < a.shape[0]])\n",
    "    return out\n",
    "\n",
    "\n",
    "MAX_CONV = 4\n",
    "TPB = 8\n",
    "TPB_MAX_CONV = TPB + MAX_CONV\n",
    "def conv_test(cuda):\n",
    "    def call(out, a, b, a_size, b_size) -> None:\n",
    "        \"\"\"\n",
    "        # i \\in (0, thread_num + blockDim), 所以i可能大于线程数, 表示block[i > 0]下的线程组\n",
    "        # local_i \\in (0, thread_num), shared只能用`local_i`索引, 表示按block独立\n",
    "        \"\"\"\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 17 lines)\n",
    "        \"\"\" ## v1\n",
    "        shared = cuda.shared.array(TPB, numba.float32)\n",
    "        if i < a_size:\n",
    "            tmp = 0\n",
    "            gap = \n",
    "            offset = min(b_size, gap)\n",
    "            for j in range(offset):\n",
    "                tmp += a[local_i + j] * b[j]\n",
    "            shared[local_i] = tmp\n",
    "            cuda.syncthreads()\n",
    "            out[i] = shared[local_i]\n",
    "        \"\"\"\n",
    "        shared = cuda.shared.array(TPB_MAX_CONV, numba.float32)  # saving [array; conv]\n",
    "        if i < a_size:\n",
    "            shared[local_i] = a[i]\n",
    "            if local_i < b_size:\n",
    "                shared[TPB + local_i] = b[local_i]\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            tmp = 0\n",
    "            for j in range(b_size):\n",
    "                if local_i + j < min(a_size, TPB):  # NOTE `shared[local_i + j]` might be out of bounds for `a`\n",
    "                    tmp += shared[local_i + j] * shared[TPB + j]  # tmp += 1  ## DEBUG\n",
    "\n",
    "            # 特殊处理: For test 2, thread-0共享不到a[8:10], 所以卷积后k位都是0\n",
    "            # 注视掉这部分不影响 test 1, 会影响 test 2的中间部分out[5:7]统计\n",
    "            if local_i == i and TPB < a_size and TPB - local_i < MAX_CONV:\n",
    "                for j in range(TPB - local_i, MAX_CONV):  # 1, 2, 3\n",
    "                    tmp += (shared[local_i] + j) * j\n",
    "\n",
    "            # if cuda.blockIdx.x == 0: Test 1 only part\n",
    "            out[i] = tmp\n",
    "        \n",
    "    return call\n",
    "\n",
    "\n",
    "# Test 1\n",
    "\n",
    "SIZE = 6\n",
    "CONV = 3\n",
    "out = np.zeros(6)\n",
    "a = np.arange(6)\n",
    "b = np.arange(3)\n",
    "problem = CudaProblem(\n",
    "    \"1D Conv (Simple)\",\n",
    "    conv_test,\n",
    "    [a, b],\n",
    "    out,\n",
    "    [SIZE, CONV],\n",
    "    Coord(1, 1),\n",
    "    Coord(TPB, 1),\n",
    "    spec=conv_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out = np.zeros(15)\n",
    "a = np.arange(15)\n",
    "b = np.arange(4)\n",
    "problem = CudaProblem(\n",
    "    \"1D Conv (Full)\",\n",
    "    conv_test,\n",
    "    [a, b],\n",
    "    out,\n",
    "    [15, 4],\n",
    "    Coord(2, 1),\n",
    "    Coord(TPB, 1),\n",
    "    spec=conv_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 12 - Prefix Sum\n",
    "\n",
    "Implement a kernel that computes a sum over `a` and stores it in `out`.\n",
    "If the size of `a` is greater than the block size, only store the sum of\n",
    "each block."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will do this using the [parallel prefix sum](https://en.wikipedia.org/wiki/Prefix_sum) algorithm in shared memory.\n",
    "That is, each step of the algorithm should sum together half the remaining numbers.\n",
    "Follow this diagram:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://user-images.githubusercontent.com/35882/178757889-1c269623-93af-4a2e-a7e9-22cd55a42e38.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TPB = 8\n",
    "def sum_spec(a):\n",
    "    out = np.zeros((a.shape[0] + TPB - 1) // TPB)\n",
    "    for j, i in enumerate(range(0, a.shape[-1], TPB)):\n",
    "        out[j] = a[i : i + TPB].sum()\n",
    "    return out\n",
    "\n",
    "\n",
    "def sum_test(cuda):\n",
    "    def call(out, a, size: int) -> None:\n",
    "        cache = cuda.shared.array(TPB, numba.float32)\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "        # FILL ME IN (roughly 12 lines)\n",
    "\n",
    "        depth = math.ceil(math.log2(TPB))  # 最大分组数: TPB决定了数组长度, 对数组做二分\n",
    "\n",
    "        if i < size:\n",
    "            cache[local_i] = a[i]\n",
    "            cuda.syncthreads()\n",
    "            ### print(\"Block and Thread: \", cuda.blockIdx.x, local_i)\n",
    "            # d := (0, 1, 2, 3) => cache[i] += cache[i + (2 ** d)] if local_i % step == 0 ## 即只算a[index]为偶数的位置\n",
    "            for d in range(depth):\n",
    "                step = 2 ** (d + 1)\n",
    "                move = 2 ** d\n",
    "                if local_i % step == 0:\n",
    "                    cache[local_i] = cache[local_i] + cache[local_i + move]\n",
    "                cuda.syncthreads()\n",
    "             \n",
    "            out[cuda.blockIdx.x] = cache[0]\n",
    "    return call\n",
    "\n",
    "\n",
    "# Test 1\n",
    "\n",
    "SIZE = 8\n",
    "out = np.zeros(1)\n",
    "inp = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Sum (Simple)\",\n",
    "    sum_test,\n",
    "    [inp],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    Coord(1, 1),\n",
    "    Coord(TPB, 1),\n",
    "    spec=sum_spec,\n",
    ")\n",
    "\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SIZE = 15\n",
    "out = np.zeros(2)\n",
    "inp = np.arange(SIZE)\n",
    "problem = CudaProblem(\n",
    "    \"Sum (Full)\",\n",
    "    sum_test,\n",
    "    [inp],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    Coord(2, 1),\n",
    "    Coord(TPB, 1),\n",
    "    spec=sum_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 13 - Axis Sum\n",
    "\n",
    "Implement a kernel that computes a sum over each column of `a` and stores it in `out`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TPB = 8\n",
    "def sum_spec(a):\n",
    "    out = np.zeros((a.shape[0], (a.shape[1] + TPB - 1) // TPB))\n",
    "    for j, i in enumerate(range(0, a.shape[-1], TPB)):\n",
    "        out[..., j] = a[..., i : i + TPB].sum(-1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def axis_sum_test(cuda):\n",
    "    def call(out, a, size: int) -> None:\n",
    "        cache = cuda.shared.array(TPB, numba.float32)\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        local_i = cuda.threadIdx.x\n",
    "        batch = cuda.blockIdx.y\n",
    "        # FILL ME IN (roughly 12 lines)\n",
    "        if i < size:\n",
    "            cache[local_i] = a[batch, i]\n",
    "            cuda.syncthreads()\n",
    "            \n",
    "            depth = math.ceil(math.log2(TPB))\n",
    "            \n",
    "            for j in range(depth):\n",
    "                step = 2 ** (j + 1)\n",
    "                move = 2 ** j\n",
    "                if local_i % step == 0:\n",
    "                    cache[local_i] = cache[local_i] + cache[local_i + move]\n",
    "                    cuda.syncthreads()\n",
    "\n",
    "            out[batch, cuda.blockIdx.x] = cache[0]\n",
    "    return call\n",
    "\n",
    "\n",
    "BATCH = 4\n",
    "SIZE = 6\n",
    "out = np.zeros((BATCH, 1))\n",
    "inp = np.arange(BATCH * SIZE).reshape((BATCH, SIZE))\n",
    "problem = CudaProblem(\n",
    "    \"Axis Sum\",\n",
    "    axis_sum_test,\n",
    "    [inp],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    Coord(1, BATCH),\n",
    "    Coord(TPB, 1),\n",
    "    spec=sum_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Puzzle 14 - Matrix Multiply!\n",
    "\n",
    "Implement a kernel that multiplies square matrices `a` and `b` and\n",
    "stores the result in `out`.\n",
    "\n",
    "*Tip: The most efficient algorithm here will copy a block into\n",
    " shared memory before computing each of the individual row-column\n",
    " dot products. This is easy to do if the matrix fits in shared\n",
    " memory.  Do that case first. Then update your code to compute\n",
    " a partial dot-product and iteratively move the part you\n",
    " copied into shared memory.* You should be able to do the hard case\n",
    " in 6 global reads."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def matmul_spec(a, b):\n",
    "    return a @ b\n",
    "\n",
    "\n",
    "TPB = 3\n",
    "def mm_oneblock_test(cuda):\n",
    "    def call(out, a, b, size: int) -> None:\n",
    "        a_shared = cuda.shared.array((TPB, TPB), numba.float32)\n",
    "        b_shared = cuda.shared.array((TPB, TPB), numba.float32)\n",
    "\n",
    "        i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        j = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "        local_i = cuda.threadIdx.x\n",
    "        local_j = cuda.threadIdx.y\n",
    "\n",
    "        # FILL ME IN (roughly 14 lines)\n",
    "        \n",
    "        \"\"\" ### 计算矩阵分块数:\n",
    "        # `a`和`b`是全局变量, 各线程均可访问, 因此可\n",
    "        # 1. 通过`cache`存储每对block的dot值\n",
    "        # 2. 通过`tmp`维护多对block的dot累加结果 \"\"\"\n",
    "        tmp = 0            \n",
    "        BLOCK = math.ceil(size / TPB)\n",
    "        for b_id in range(BLOCK):\n",
    "                \n",
    "            block_i = b_id * cuda.blockDim.x + cuda.threadIdx.x\n",
    "            block_j = b_id * cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "            # NOTE 不能同时判断`block_i`和`block_j`, 因为block2不是方阵 !\n",
    "            # 基本同理, `i < size`和`j < size`不能放在外层判断, 否则:\n",
    "            # e.g. `j == 8`对应: (blocky = 2 + thready = 2) \n",
    "            #      如果下哦那个了面这段用了`i < size and j < size` 那只能取到一个方阵`a[k, k]`的值\n",
    "            #      所以a[k, 3]的值无法获取到\n",
    "            #      通过下面的**print**可观察到区别\n",
    "            # if i < size and j < size and block_j < size:\n",
    "            #     a_shared[local_i, local_j] = a[i, block_j]\n",
    "            # if i == 7 and j == 8 and block_j == 2:\n",
    "            #     print(a_shared[0, 0], a_shared[0, 1], a_shared[0, 2])\n",
    "            #     print(a_shared[1, 0], a_shared[1, 1], a_shared[1, 2])\n",
    "            #     print(a_shared[2, 0], a_shared[2, 1], a_shared[2, 2])\n",
    "\n",
    "            if i < size and block_j < size:\n",
    "                a_shared[local_i, local_j] = a[i, block_j]\n",
    "            if block_i < size and j < size:\n",
    "                b_shared[local_i, local_j] = b[block_i, j]\n",
    "\n",
    "            cuda.syncthreads()\n",
    "\n",
    "            for k in range(min(size - b_id * TPB, TPB)):\n",
    "                tmp += a_shared[local_i, k] * b_shared[k, local_j]  # a[i, .] * b[., j]\n",
    "\n",
    "        if i < size and j < size:\n",
    "            out[i, j] = tmp\n",
    "\n",
    "    return call\n",
    "\n",
    "# Test 1\n",
    "\n",
    "SIZE = 2\n",
    "out = np.zeros((SIZE, SIZE))\n",
    "inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T\n",
    "\n",
    "problem = CudaProblem(\n",
    "    \"Matmul (Simple)\",\n",
    "    mm_oneblock_test,\n",
    "    [inp1, inp2],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    Coord(1, 1),\n",
    "    Coord(TPB, TPB),\n",
    "    spec=matmul_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show(sparse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SIZE = 8\n",
    "out = np.zeros((SIZE, SIZE))\n",
    "inp1 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "inp2 = np.arange(SIZE * SIZE).reshape((SIZE, SIZE)).T\n",
    "\n",
    "problem = CudaProblem(\n",
    "    \"Matmul (Full)\",\n",
    "    mm_oneblock_test,\n",
    "    [inp1, inp2],\n",
    "    out,\n",
    "    [SIZE],\n",
    "    Coord(3, 3),\n",
    "    Coord(TPB, TPB),\n",
    "    spec=matmul_spec,\n",
    ")\n",
    "problem.check()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "problem.show(sparse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}